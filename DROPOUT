import pandas as pd
import lightgbm as lgb
import numpy as np
import random
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

def dropout_lgbm(cant_features_eliminar, X_train, y_train, X_test, y_test, X_OOB, y_OOB):
    # Inicializar una lista para almacenar los resultados
    results_list = []

    # Definir el DataFrame para almacenar los resultados
    results_df = pd.DataFrame(columns=['Feature Dropped', 'AUC Score (X_OOB)', 'AUC Score (X_test)'])

    # Entrenar el modelo inicial
    dtrain = lgb.Dataset(
        data=X_train,
        label=y_train,
        weight=np.where(y_train.isin([0]), 1.1, 1.0),
        free_raw_data=False
    )

    parameters = {
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'learning_rate': 0.05,
        'predict_disable_shape_check': True,
        'num_boost_round': 100,  # Añadido como ejemplo
    }

    model = lgb.train(
        params=parameters,
        train_set=dtrain
    )

    # Calcular el AUC inicial en el conjunto X_OOB
    y_pred_initial_OOB = model.predict(X_OOB)
    initial_auc_OOB = roc_auc_score(y_OOB, y_pred_initial_OOB)

    # Calcular el AUC inicial en el conjunto X_test
    y_pred_initial_test = model.predict(X_test)
    initial_auc_test = roc_auc_score(y_test, y_pred_initial_test)

    results_list.append({'Feature Dropped': 'None', 'AUC Score (X_OOB)': initial_auc_OOB, 'AUC Score (X_test)': initial_auc_test})

    for _ in range(1000):
        # Seleccionar características aleatorias para eliminar
        features_to_drop = random.sample(X_train.columns.tolist(), cant_features_eliminar)
        
        # Crear una copia del DataFrame sin las variables seleccionadas
        X_train_dropped = X_train.drop(columns=features_to_drop)
        X_test_dropped = X_test.drop(columns=features_to_drop)
        
        dtrain_dropped = lgb.Dataset(
            data=X_train_dropped,
            label=y_train,
            weight=np.where(y_train.isin([0]), 1.1, 1.0),
            free_raw_data=False
        )
        
        model_dropped = lgb.train(
            params=parameters,
            train_set=dtrain_dropped
        )
        
        y_pred_dropped_OOB = model_dropped.predict(X_OOB[X_train_dropped.columns])
        auc_dropped_OOB = roc_auc_score(y_OOB, y_pred_dropped_OOB)
        
        y_pred_dropped_test = model_dropped.predict(X_test_dropped)
        auc_dropped_test = roc_auc_score(y_test, y_pred_dropped_test)
        
        results_list.append({'Feature Dropped': ', '.join(features_to_drop), 'AUC Score (X_OOB)': auc_dropped_OOB, 'AUC Score (X_test)': auc_dropped_test})

    results_df = pd.DataFrame(results_list)
    return results_df

def plot_results(results_df, limite_superior_AUC):
    condition = (results_df['AUC Score (X_OOB)'] > limite_superior_AUC) & (results_df['AUC Score (X_test)'] > limite_superior_AUC) & \
        (results_df['Feature Dropped'].str.contains('model_price')) & \
        (results_df['Feature Dropped'].str.contains('model_price_new_1'))

    filtered_combinations = results_df[condition]

    plt.figure(figsize=(10, 6))
    plt.scatter(results_df['AUC Score (X_OOB)'], results_df['AUC Score (X_test)'], alpha=0.5, label='Combinaciones')
    plt.scatter(filtered_combinations['AUC Score (X_OOB)'], filtered_combinations['AUC Score (X_test)'], c='red', alpha=0.7, label='Cumplen Condiciones')
    plt.xlabel('AUC Score (X_OOB)')
    plt.ylabel('AUC Score (X_test)')
    plt.legend()
    plt.show()

# Imagino que en algún punto tienes la definición de tus variables X_train, y_train, etc. 
results_df = dropout_lgbm(10, X_train, y_train, X_test, y_test, X_OOB, y_OOB)
plot_results(results_df, limite_superior_AUC=0.6)

